{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40e05fe5",
   "metadata": {},
   "source": [
    "# Knowing your task and knowing your data.\n",
    "\n",
    "- While building your machine learning algorithm, keep in mind the following questions.\n",
    "1. What question/s am I trying to answer?\n",
    "2. Do I think the data collected can answer that question?\n",
    "3. What is the best way to phrase my question as a machine learning problem?\n",
    "4. Have I collected enough data to represent the problem I want to solve?\n",
    "5. What features did I extract? Will these enable the right predictions?\n",
    "6. How will I measure success in my application?\n",
    "7. How will the machine learning solution interact with other parts of my research or business product?\n",
    "\n",
    "- In a larger context, algorithms and methods in machine learning are only one part of a greater process to solve a particular problem. \n",
    "- It is a good idea to keep the big picture in mind.\n",
    "- Many people spend a lot of time building a complex model, only to find out it doesn't solve the right problem.\n",
    "- When going deep into technical aspects of machine learning, it is easy to lose sight of the ultimate goals.\n",
    "- Keep in mind all of the assumptions that you make, explicitly or implicitly when you start building machine learning models.\n",
    "\n",
    "## Why Python?\n",
    "1. Combines the powerful general purpose programming\n",
    "2. Ease of use for domain specific languages like MATLAB and R\n",
    "3. Many libraries for data loading, visualisation, statistics, language processing, image processing, plotting and more\n",
    "4. Machine learning and data analysis are fundamentally iterative processes, in which the data drives the analysis. \n",
    "5. It's essential for these processes to have tools that allow quick iteration and easy of interaction.\n",
    "6. Python also integrates into existing systems easily, web services, data bases etc.\n",
    "\n",
    "## Scikit-learn\n",
    "1. Is an open source project\n",
    "2. Constantly being developed and improved, very active community\n",
    "3. Contains a number of state of the art machine learning algorithms\n",
    "4. Documentation \n",
    "\n",
    "https://scikit-learn.org/stable/user_guide.html\n",
    "\n",
    "\n",
    "## Python Libraries\n",
    "- NumPy: Library for numerical operations and array manipulations in Python.\n",
    "- SciPy: Library for scientific computing, built on NumPy, offering additional functions.\n",
    "- matplotlib: Plotting library for creating static, interactive, and animated visualizations.\n",
    "- pandas: Library for data manipulation and analysis, especially with tabular data.\n",
    "- IPython: Enhanced interactive Python shell providing powerful introspection, rich media, and more.\n",
    "- Jupyter Notebook: Web-based interactive environment for creating and sharing documents with live code.\n",
    "- scikit-learn: Machine learning library for Python, providing tools for data mining and analysis.\n",
    "\n",
    "pip install numpy scipy matplotlib ipython scikit-learn pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bbd504",
   "metadata": {},
   "source": [
    "### NumPy\n",
    "- Fundamental package for scientific computing in Python. \n",
    "- Contains functionality for multi-dimensional arrays\n",
    "- High level linear algebra operations\n",
    "- Fourier transform used for signal processing\n",
    "- In SciKit-Learn The NumPy array is the fundamental data structure\n",
    "  - The class is ndarray\n",
    "  - A multi-dimensional (n-dimensional) array\n",
    "  - All elements must be of the same type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a55a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "x = np.array([[1,2,3], [4,5,6]])\n",
    "print(x)\n",
    "display(x)\n",
    "print(f\"x:\\n{x}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521fd0d0",
   "metadata": {},
   "source": [
    "### SciPy\n",
    "- Collection of functions for scientific computing in Python.\n",
    "- Advanced linear algebra routines\n",
    "- Mathematic function optimization\n",
    "- Signal processing\n",
    "- Statistical distributions\n",
    "- SciKit-learn draws from scipy's functions for implementing it's algorithms.\n",
    "- The most important part of SciPy for this book is scipy.sparse\n",
    "  - This provides sparse matrices\n",
    "  - Which are another representation that is used for data in scikit-learn\n",
    "  - We use a spare matrix when we want to store a 2D array that contains mainly zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfeff16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "  (0, 0)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 2)\t1.0\n",
      "  (3, 3)\t1.0\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "eye = np.eye(4)\n",
    "print(eye)\n",
    "sparse_matrix = sparse.csr_matrix(eye)\n",
    "print(sparse_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b257c960",
   "metadata": {},
   "source": [
    "- Creating a sparse matrix in COO format.\n",
    "- COO means Coordinate \n",
    "- It stores a list of the non-zero elements in a matrix, along with their row and column index. \n",
    "- It creates three 1D arrays.\n",
    "  - Row Index\n",
    "  - Column Index\n",
    "  - Data\n",
    "- It's useful for constructing sparse matrices and converting to other formats like Compressed Sparse Row, or Compressed Spare Column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da296cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 2)\t1.0\n",
      "  (3, 3)\t1.0\n"
     ]
    }
   ],
   "source": [
    "data = np.ones(4)\n",
    "row_indices = np.arange(4)\n",
    "col_indices = np.arange(4)\n",
    "eye_coo = sparse.coo_matrix((data, (row_indices, col_indices)))\n",
    "print(f\"{eye_coo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8291f82",
   "metadata": {},
   "source": [
    "### Matplotlib\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292ed9f2-58fb-406d-badc-de7af84db2f0",
   "metadata": {},
   "source": [
    "## Meet the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581bdd5f-2000-4d2c-ae0f-2140b35c61a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris_dataset = load_iris()\n",
    "print(iris_dataset.keys())\n",
    "print(dir(iris_dataset))\n",
    "# print(iris_dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e2a163-35f5-43c4-915d-6d605af53783",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris_dataset[\"target_names\"])\n",
    "print(iris_dataset[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dba221-ae8f-484c-8bf1-ced929b4c353",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris_dataset[\"feature_names\"])\n",
    "print(iris_dataset[\"data\"][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e905445-db93-4273-884e-bb9a59d4ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris_dataset['data'].shape)\n",
    "print(iris_dataset['target'].shape)\n",
    "print(iris_dataset['target_names'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0cef63-ddb8-491a-9199-01538444705c",
   "metadata": {},
   "source": [
    "We see that the array contains measurements for 150 different flowers. Remember\n",
    "that the individual items are called samples in machine learning, and their properties\n",
    "are called features. The shape of the data array is the number of samples multiplied by\n",
    "the number of features. This is a convention in scikit-learn, and your data will\n",
    "always be assumed to be in this shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecf989f-10ac-4dfa-b79e-6a54a9c8ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(iris_dataset[\"data\"]))\n",
    "print(type(iris_dataset.data))\n",
    "print(iris_dataset[\"data\"])\n",
    "print(iris_dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d41475-15b2-4b29-b59e-a62fadc9a8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris_dataset[\"data\"][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc8b88d-e163-4a30-bc4d-060852d73839",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris_dataset['target'].shape)\n",
    "print(iris_dataset.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f937ab-0e03-4eb7-928d-eab16b013da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(iris_dataset['target']))\n",
    "print(type(iris_dataset.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5882c993-565f-416f-9a23-f548fa942a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris_dataset[\"target\"][:5])\n",
    "print(iris_dataset.target[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba865fcd-3c48-46bc-9498-738e231cc21a",
   "metadata": {},
   "source": [
    "The meanings of the numbers are given by the iris['target_names'] array:\n",
    "0 means setosa, 1 means versicolor, and 2 means virginica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a435adcc-cffd-4b19-bafb-5b2b9a351594",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris_dataset.feature_names)\n",
    "print(iris_dataset.data[1])\n",
    "print(iris_dataset.target[1])\n",
    "print(iris_dataset.target_names[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d7356f-845f-4bd9-80c4-f9f5020085e9",
   "metadata": {},
   "source": [
    "## Measuring Success: Training and Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d911d9cb-7af1-4640-af72-c2279887fef3",
   "metadata": {},
   "source": [
    "train_test_split function. This function extracts 75% of the rows in the data as the\n",
    "training set, together with the corresponding labels for this data. The remaining 25%\n",
    "of the data, together with the remaining labels, is declared as the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30d2d89-a385-425a-a728-2055548d68a4",
   "metadata": {},
   "source": [
    "In scikit-learn, data is usually denoted with a capital X, while labels are denoted by\n",
    "a lowercase y. This is inspired by the standard formulation f(x)=y in mathematics,\n",
    "where x is the input to a function and y is the output. Following more conventions\n",
    "from mathematics, we use a capital X because the data is a two-dimensional array (a\n",
    "matrix) and a lowercase y because the target is a one-dimensional array (a vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f504a9a4-177b-4088-ad0d-8cb6a733de64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "help(train_test_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02930419",
   "metadata": {},
   "source": [
    "# train_test_split() random_state parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6458b360-7149-4b52-9fc2-1701f4e23862",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris_dataset['data'], iris_dataset['target'])\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(X_train[0])\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1dd7e7-0956-46be-a745-3c38b27392cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris_dataset['data'], iris_dataset['target'], random_state=0)\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(X_train[0])\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8570f93d-44da-4e62-85c0-d664c14fe0ec",
   "metadata": {},
   "source": [
    "## First Things First: Look at Your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6716aa64-a17b-4c6e-8969-036da7e028b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository (you only need to do this once)\n",
    "# git clone https://github.com/amueller/introduction_to_ml_with_python.git\n",
    "\n",
    "# Install the dependencies (you only need to do this once)\n",
    "# pip install -r introduction_to_ml_with_python/requirements.txt\n",
    "\n",
    "# Set up the path and import mglearn\n",
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\James\\\\Desktop\\\\RoboticsJourney\\\\Self_Study\\\\2.PatternReg.ML\\\\introduction_to_ml_with_python')\n",
    "import mglearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "X_train: np.ndarray\n",
    "X_test: np.ndarray\n",
    "y_train: np.ndarray\n",
    "y_test: np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135476de-93c9-4e4e-9021-a685e6069a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe from data in X_train\n",
    "# label the columns using the strings in iris_dataset.feature_names\n",
    "iris_dataframe = pd.DataFrame(X_train, columns = iris_dataset.feature_names)\n",
    "\n",
    "# create a scatter matrix from the dataframe, color by y_train\n",
    "grr = pd.plotting.scatter_matrix(iris_dataframe, c=y_train, figsize=(15, 15), marker='o',\n",
    "hist_kwds={'bins': 20}, s=60, alpha=.8, cmap=mglearn.cm3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8873ae0e-c23d-4654-a81f-254c131ae0b2",
   "metadata": {},
   "source": [
    "## Building Your First Model: k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2875687b-0630-4b2b-86af-d4cfa6060ce9",
   "metadata": {},
   "source": [
    "There are many classification \n",
    "algorithms in scikit-learn that we could use. Here we will use a k-neares \n",
    "neighbors classifier, which is easy to understand. Building this model only consists of\n",
    "storing the training set. To make a prediction for a new data point, the algorithm\n",
    "finds the point in the training set that is closest to the new point. Then it assigns the\n",
    "label of this training point to the new data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc31453-08ac-4f59-96ef-2886333a6a9b",
   "metadata": {},
   "source": [
    "Models in scikit-learn are implemented in their own classes,\n",
    "which are called Estimator classes. The k-nearest neighbors classification algorithm\n",
    "is implemented in the KNeighborsClassifier class in the neighbors module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e78db9-d87e-45dc-9adf-46393e2e4c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cefe11-5d68-4120-babb-83ba5e4bbe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(knn)\n",
    "dir(knn)\n",
    "help(knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35306e16-82af-45ca-bb25-8fc6687f417a",
   "metadata": {},
   "source": [
    "To build the model on the training set, we call the fit method of the knn object,\n",
    "which takes as arguments the NumPy array X_train containing the training data and\n",
    "the NumPy array y_train of the corresponding training labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c638c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris_dataset['data'], iris_dataset['target'], random_state=0)\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(X_train[0])\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5964c92f-63f0-41c4-867e-921c0ce6e79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d6a285-60ed-40a7-b7e0-cbfb43807b80",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8300155-5b8d-4e47-bf27-4e900768f851",
   "metadata": {},
   "source": [
    "Imagine we found an iris in the wild with the following:  \n",
    "- Sepal length = 5cm\n",
    "- Sepal width = 2.9cm\n",
    "- Petal length = 1cm\n",
    "- Petal width 0.2cm  \n",
    "\n",
    "Note: \n",
    "- A two-dimensional array is a matrix  \n",
    "- A one-dimensional array is a vector  \n",
    "- scikit-learn always expects two-dimensional arrays for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e675030f-3501-4ee8-9490-d6147741360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[5, 2.9, 1, 0.2]])\n",
    "print(X_new.shape)\n",
    "print(X_new)\n",
    "type(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25c3264-5e7a-4cca-b8c2-9cb3d537350e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Printing the iris_dataset bunch object elements: {dir(iris_dataset)}\")\n",
    "\n",
    "prediction = knn.predict(X_new)\n",
    "print(f\"The prediction value is {prediction}\")\n",
    "\n",
    "print(f\"Which matches the species: {iris_dataset.target_names[prediction]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35326e23-290f-45c5-be8b-2357e4149940",
   "metadata": {},
   "source": [
    "This new iris belongs to the class 0, meaning its species is setosa. But how do we know whether we can trust our model?  \n",
    "We don’t know the correct species of this sample, which is the whole point of building the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0cf1ae-d7b3-46de-8e36-7a91e8e8339b",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f911b90e-92d7-4024-ad00-0961d10b1c79",
   "metadata": {},
   "source": [
    "Make a prediction for each iris in the test data and compare it against its label (the known species).  \n",
    "We can measure how well the model works by computing the accuracy, which is the fraction of flowers for which the right species was predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dc7c67-9630-458d-a61b-2de4c044e1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# npArray = np.array([[5, 2.9, 1, 0.2]])\n",
    "# print(f\"npArray type: {type(npArray)}\")\n",
    "# print(f\"npArray shape: {npArray.shape}\")\n",
    "\n",
    "X_train: np.ndarray\n",
    "X_test: np.ndarray\n",
    "y_train: np.ndarray\n",
    "y_test: np.ndarray\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_dataset['data'], iris_dataset['target'], random_state=0)\n",
    "\n",
    "print(f\"X_test class: {type(X_test)}\")\n",
    "print(f\"X_test shape is a matrix: {X_test.shape}\")\n",
    "print(f\"X_test first element is: {X_test[0]}\\n\")\n",
    "\n",
    "print(f\"y_test class: {type(y_test)}\")\n",
    "print(f\"y_test is a vector of labels {y_test.shape}\")\n",
    "print(f\"y_test first element is: {y_test[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bc3cde-5245-46bf-9e14-b6bcb32c646c",
   "metadata": {},
   "source": [
    "# Sample Definition\n",
    "A sample, observation or instance is a single data point or record in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b20723",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"1. Because scikit-learn has bunch object, we need to display the hash table properties this way first:\\n\\t{dir(iris_dataset)}\\n\")\n",
    "print(f\"2. The first sample, observation, instance or single data point is: {iris_dataset.data[0]} and these are the \\\"features\\\" of the sample\\n\")\n",
    "print(f\"3. The headings for these features are:\\n\\t{iris_dataset.feature_names}\\n\")\n",
    "print(f\"4. The first sample shown in step 2 has a \\\"class\\\" value, this is: {iris_dataset.target[0]} and is the \\\"target\\\" property\\n\")\n",
    "print(f\"5. The headings for the \\\"classes\\\" are:\\n\\t{iris_dataset.target_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c8c677-4b05-4d87-8995-ce2c8a859d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_test is loaded into the model and predicted against. The output contains the \\\"target\\\" predictions, so we store them in y_pred\\n\")\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(f\"{'X_test':<50} {'y_pred'}\")\n",
    "for x, y in zip(X_test, y_pred):\n",
    "    print(f\"{str(x):<50} {y}\")\n",
    "\n",
    "\n",
    "# df = pd.DataFrame(X_test, columns = iris_dataset['feature_names'])\n",
    "# df['Predicted'] = y_pred\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ca1421-4b36-4eb2-807c-a6f3a5828cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The score of accuracy of our model against the Training data is: {knn.score(X_train, y_train)}\")\n",
    "print(f\"The score of accuracy of our model against the Testing data is: {knn.score(X_test, y_test)}\\n\")\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "print(f\"Finding the mean between y_test and y_pred: {np.mean(y_test == y_pred)}\\n\")\n",
    "\n",
    "# print(f\"The score of accuracy of our model with X_test and y_pred is: {knn.score(X_train, y_pred)}\")\n",
    "print(f\"The score of accuracy of our model with X_test and y_pred is: {knn.score(X_test, y_pred)}\")\n",
    "\n",
    "print(f\"\\nOkay I think I get it now\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
