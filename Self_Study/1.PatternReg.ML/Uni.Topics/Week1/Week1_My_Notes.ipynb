{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week one lecture and topic notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are practical examples of how both linear and quadratic classifiers might be used in real-world applications:\n",
    "\n",
    "### 1. **Linear Classifier Example: Spam Detection**\n",
    "   - **Scenario:** In email spam detection, a linear classifier can be used to separate spam from non-spam emails.\n",
    "   - **Features:** Words or phrases in the email, email length, presence of certain keywords (e.g., \"free,\" \"win\").\n",
    "   - **Decision Boundary:** The classifier might use a hyperplane to classify emails as either spam or not based on the presence of certain words.\n",
    "   - **Why Linear?** Spam vs. non-spam can often be separated using a combination of features that have a linear relationship, such as the frequency of specific spam-indicating words.\n",
    "   - **Classifier Used:** Logistic regression or linear Support Vector Machine (SVM).\n",
    "   - **Outcome:** The model might learn that emails containing the words \"free\" and \"money\" above certain thresholds are classified as spam, while others are not.\n",
    "\n",
    "### 2. **Quadratic Classifier Example: Handwriting Recognition**\n",
    "   - **Scenario:** In handwriting recognition (e.g., recognizing digits in the MNIST dataset), a quadratic classifier might be used to distinguish between the handwritten digits.\n",
    "   - **Features:** Pixel intensity values for different regions of the digit images.\n",
    "   - **Decision Boundary:** A quadratic decision boundary allows the model to better differentiate between classes, such as the digit \"0\" and the digit \"8,\" which might have overlapping shapes in certain linear projections.\n",
    "   - **Why Quadratic?** The shapes of different handwritten digits are not linearly separable, meaning you need more flexible decision boundaries that can curve around clusters of data.\n",
    "   - **Classifier Used:** Quadratic Discriminant Analysis (QDA).\n",
    "   - **Outcome:** The quadratic classifier can model the more complex relationships between pixel intensities that characterize different digits, offering better accuracy than a linear model.\n",
    "\n",
    "### Summary of Differences:\n",
    "- **Linear Classifier (Spam Detection):** Works well when the features (such as word frequencies) are roughly linearly related to the output (spam or not).\n",
    "- **Quadratic Classifier (Handwriting Recognition):** Necessary when the data (like pixel intensity values) have non-linear patterns, and a curved boundary is needed to accurately classify the inputs.\n",
    "\n",
    "These examples highlight how the complexity of the data and relationships between features can guide the choice between linear and quadratic classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative and discriminative models are two fundamental approaches in machine learning, each with distinct ways of understanding and modeling data.\n",
    "\n",
    "### 1. **Generative Models**\n",
    "   - **Definition:** Generative models learn the joint probability distribution \\( P(X, Y) \\) of the input features \\( X \\) and the labels \\( Y \\). Once this distribution is learned, the model can be used to generate new data points or classify by estimating \\( P(Y|X) \\) using Bayes' Theorem.\n",
    "   - **How They Work:**\n",
    "     - They model how the data was generated.\n",
    "     - First, they learn the distribution of the input features \\( P(X|Y) \\) for each class \\( Y \\), and the prior probability \\( P(Y) \\).\n",
    "     - Then, for a new input \\( X \\), they compute the posterior probability \\( P(Y|X) \\) using Bayes' Theorem and choose the label with the highest probability.\n",
    "   - **Common Applications:**\n",
    "     - **Data generation:** Creating new examples of images, text, or other data.\n",
    "     - **Density estimation:** Learning the distribution of the data.\n",
    "     - **Anomaly detection:** Identifying instances that don't fit the learned distribution.\n",
    "   - **Examples of Generative Models:**\n",
    "     - Naive Bayes Classifier\n",
    "     - Hidden Markov Models (HMMs)\n",
    "     - Gaussian Mixture Models (GMMs)\n",
    "     - Variational Autoencoders (VAEs)\n",
    "     - Generative Adversarial Networks (GANs)\n",
    "\n",
    "   **Pros:**\n",
    "   - Can generate new data points.\n",
    "   - Can handle missing data in some cases by modeling the joint distribution.\n",
    "   \n",
    "   **Cons:**\n",
    "   - Generally more computationally intensive.\n",
    "   - May not always lead to better classification performance compared to discriminative models.\n",
    "\n",
    "### 2. **Discriminative Models**\n",
    "   - **Definition:** Discriminative models directly model the conditional probability \\( P(Y|X) \\), i.e., the probability of the labels \\( Y \\) given the features \\( X \\), without attempting to model how the data was generated. They focus purely on the boundary between different classes.\n",
    "   - **How They Work:**\n",
    "     - They focus on distinguishing between classes.\n",
    "     - They do not model the input data distribution \\( P(X) \\); instead, they learn to map inputs \\( X \\) to labels \\( Y \\) as directly as possible.\n",
    "   - **Common Applications:**\n",
    "     - **Classification tasks:** Where the goal is to assign labels to given data points.\n",
    "     - **Regression tasks:** Where the model predicts continuous values.\n",
    "   - **Examples of Discriminative Models:**\n",
    "     - Logistic Regression\n",
    "     - Support Vector Machines (SVMs)\n",
    "     - Decision Trees\n",
    "     - Random Forests\n",
    "     - Neural Networks (when used for classification)\n",
    "\n",
    "   **Pros:**\n",
    "   - Often simpler and more efficient for classification tasks.\n",
    "   - Directly optimized for classification, often leading to better performance.\n",
    "   \n",
    "   **Cons:**\n",
    "   - Cannot generate new data.\n",
    "   - Require full data to be present (no natural handling of missing data).\n",
    "\n",
    "### Key Differences:\n",
    "| Aspect                 | **Generative Models**                              | **Discriminative Models**                         |\n",
    "|------------------------|---------------------------------------------------|---------------------------------------------------|\n",
    "| **What They Learn**     | Joint distribution \\( P(X, Y) \\)                  | Conditional distribution \\( P(Y|X) \\)             |\n",
    "| **Purpose**             | Can generate new data, model full distribution    | Classify data, focused on decision boundaries     |\n",
    "| **Examples**            | Naive Bayes, GANs, VAEs, HMMs                     | Logistic Regression, SVMs, Decision Trees         |\n",
    "| **Advantages**          | Can model data generation process, handle missing data | Typically better at classification tasks          |\n",
    "| **Disadvantages**       | More complex, not always best for classification  | Cannot generate new data                          |\n",
    "\n",
    "### Practical Example:\n",
    "- **Generative Model Example:** A **GAN** (Generative Adversarial Network) can be used to generate realistic images of faces, even creating new, previously unseen faces by learning the joint distribution of pixel values and classes.\n",
    "  \n",
    "- **Discriminative Model Example:** A **Support Vector Machine** (SVM) is used for classifying emails as spam or not spam. It focuses purely on distinguishing spam emails from non-spam, based on features extracted from the email, without modeling how those emails were generated.\n",
    "\n",
    "In summary, generative models are useful when you need to understand or generate data, while discriminative models are often more efficient and effective for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
